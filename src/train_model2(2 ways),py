import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import xgboost as xgb
import joblib

# ---------------------------
# STEP 1: Load Dataset
# ---------------------------
print("ðŸ”¹ Loading ML-ready dataset...")
df = pd.read_csv("ml_ready_CAN_dataset.csv")

if "Flag" in df.columns:
    df = df.drop(columns=["Flag"])

X = df.drop(columns=["Label"])
y = df["Label"]

print(f"ðŸ“Š Dataset shape: {X.shape}, Labels: {y.value_counts().to_dict()}")

# ---------------------------
# STEP 2: Train-Test Split
# ---------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ---------------------------
# STEP 3: Train XGBoost
# ---------------------------
print("ðŸš€ Training XGBoost model...")
clf = xgb.XGBClassifier(
    objective="multi:softmax",  # multi-class classification
    num_class=len(y.unique()),
    eval_metric="mlogloss",
    tree_method="hist",   # faster training
    n_estimators=200,
    max_depth=8,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

clf.fit(X_train, y_train)

# ---------------------------
# STEP 4: Evaluate
# ---------------------------
y_pred = clf.predict(X_test)

print("\nðŸ“Š Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ---------------------------
# STEP 5: Save Model
# ---------------------------
joblib.dump(clf, "can_ml_xgb_model.pkl")
print("ðŸ’¾ Model saved as can_ml_xgb_model.pkl")
